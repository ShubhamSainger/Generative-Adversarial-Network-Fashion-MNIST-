{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN (Fashion_MNIST).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_R0nKA6HXEN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(Dataset, _),(_, _) = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "_QRQy_QLFZwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e55e79-9aea-4415-840b-b0b6f7ca0492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Hi-YEqSPd3gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = Dataset / 127.5 - 1.0\n",
        "Dataset = np.expand_dims(Dataset, axis=3)"
      ],
      "metadata": {
        "id": "_PV1rCjTFhDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Discriminator\n",
        "Discriminator = tf.keras.Sequential(\n",
        "    [\n",
        "      tf.keras.Input(shape=(28,28,1)),\n",
        "      tf.keras.layers.Conv2D(64,kernel_size=5,strides=2,padding='same'),  #output dimensions 14,14,64\n",
        "      tf.keras.layers.LeakyReLU(0.2),\n",
        "      tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "      tf.keras.layers.Conv2D(128,kernel_size=5,strides=2,padding='same'), #output dimensions 7,7,128\n",
        "      tf.keras.layers.LeakyReLU(0.2),\n",
        "      tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "    ]\n",
        ")\n",
        "\n",
        "latent_dim = 100  #dimension of noise vector\n",
        "\n",
        "#Genrator\n",
        "Generator = tf.keras.Sequential(\n",
        "    [\n",
        "     tf.keras.Input(shape=(latent_dim)),\n",
        "     tf.keras.layers.Dense(7*7*256),\n",
        "     tf.keras.layers.LeakyReLU(0.2),\n",
        "     tf.keras.layers.BatchNormalization(),\n",
        "     tf.keras.layers.Reshape(target_shape=(7,7,256)),                             #output dimensions 7,7,256\n",
        "\n",
        "     tf.keras.layers.Conv2DTranspose(128,kernel_size=5,strides=1,padding='same'), #output dimensions 7,7,128\n",
        "     tf.keras.layers.LeakyReLU(0.2),\n",
        "     tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "     tf.keras.layers.Conv2DTranspose(64,kernel_size=5,strides=2,padding='same'),  #output dimensions 14,14,64\n",
        "     tf.keras.layers.LeakyReLU(0.2),\n",
        "     tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "     tf.keras.layers.Conv2DTranspose(1,kernel_size=5,strides=2,padding='same',activation='tanh') #output dimensions 28,28,1 which is our image's dimensions\n",
        "    ]\n",
        ")\n",
        "\n",
        "opt_disc = tf.keras.optimizers.Adam(1e-4)\n",
        "opt_gen = tf.keras.optimizers.Adam(1e-4)\n",
        "epoch = 50\n",
        "batchsize = 32\n",
        "loss_func = tf.keras.losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "f50k4y05ICGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "\n",
        "for epoch in range(10):\n",
        "  for index, real_image in enumerate(Dataset):\n",
        "    random_latent_vectors = tf.random.normal(shape = (batchsize,latent_dim))\n",
        "    gen_fake = Generator(random_latent_vectors)\n",
        "    actual_image = Dataset[np.random.randint(0,Dataset.shape[0],batchsize)]\n",
        "    print(\"index = \",index)\n",
        "    if index%100 == 0:\n",
        "      img = tf.keras.preprocessing.image.array_to_img(gen_fake[0])\n",
        "      img.save(f'/content/drive/MyDrive/MNIST/image{index}.png')\n",
        "\n",
        "    #training of discriminator\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      fake_loss = loss_func(tf.zeros(shape=(batchsize,1)),Discriminator(gen_fake))\n",
        "      real_loss = loss_func(tf.ones(shape=(batchsize,1)),Discriminator(actual_image))\n",
        "      total_disc_loss = (fake_loss + real_loss)/2\n",
        "    grades = disc_tape.gradient(total_disc_loss,Discriminator.trainable_weights)\n",
        "    opt_disc.apply_gradients(zip(grades,Discriminator.trainable_weights))\n",
        "    # print(len(grades))\n",
        "    print(total_disc_loss)\n",
        "    #training of Generator \n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_fake = Generator(random_latent_vectors)\n",
        "      output = Discriminator(gen_fake)\n",
        "      total_gen_loss = loss_func(tf.ones(shape=(batchsize,1)),output)\n",
        "    grades = gen_tape.gradient(total_gen_loss,Generator.trainable_weights)\n",
        "    # print(grades)\n",
        "    # print(grades)\n",
        "    print(total_gen_loss)\n",
        "    opt_gen.apply_gradients(zip(grades,Generator.trainable_weights))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JO2h1CepFp5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
